{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab2116e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/media/dnth/Active-Projects/vl-datasets/notebooks/images_dir/zh-plus___parquet/Maysee--tiny-imagenet-2eb6c3acd8ebc62a/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "Found cached dataset parquet (/media/dnth/Active-Projects/vl-datasets/notebooks/images_dir/zh-plus___parquet/Maysee--tiny-imagenet-2eb6c3acd8ebc62a/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import datasets\n",
    "\n",
    "# Load the dataset\n",
    "train_dataset = datasets.load_dataset(\"zh-plus/tiny-imagenet\", split=\"train\", cache_dir='images_dir')\n",
    "valid_dataset = datasets.load_dataset(\"zh-plus/tiny-imagenet\", split=\"valid\", cache_dir='images_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "539b51bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=64x64>,\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1d05490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomResizedCrop(64),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "valid_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(64),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def preprocess_train(example_batch):\n",
    "    \"\"\"Apply train_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [\n",
    "        train_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]\n",
    "    ]\n",
    "    return example_batch\n",
    "\n",
    "def preprocess_valid(example_batch):\n",
    "    \"\"\"Apply valid_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [\n",
    "        valid_transform(image.convert(\"RGB\")) for image in example_batch[\"image\"]\n",
    "    ]\n",
    "    return example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e7f272b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_transform(preprocess_train)\n",
    "valid_dataset.set_transform(preprocess_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c68ad5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=64x64>,\n",
       " 'label': 0,\n",
       " 'pixel_values': tensor([[[ 0.6734,  0.6563,  0.6392,  ..., -0.9363, -0.8507, -0.7822],\n",
       "          [ 0.7077,  0.6906,  0.6734,  ..., -0.9020, -0.7993, -0.7137],\n",
       "          [ 0.8104,  0.7762,  0.7419,  ..., -0.8507, -0.6623, -0.5424],\n",
       "          ...,\n",
       "          [-0.5596, -0.6281, -0.7137,  ..., -0.7822, -0.8164, -0.8335],\n",
       "          [-0.5424, -0.6109, -0.7308,  ..., -0.9020, -0.8678, -0.8507],\n",
       "          [-0.5424, -0.6109, -0.7308,  ..., -0.9534, -0.9020, -0.8678]],\n",
       " \n",
       "         [[ 0.4853,  0.4853,  0.5028,  ...,  0.1877,  0.2577,  0.3102],\n",
       "          [ 0.5028,  0.5028,  0.5028,  ...,  0.2052,  0.3102,  0.3803],\n",
       "          [ 0.5203,  0.5203,  0.5203,  ...,  0.2402,  0.4153,  0.5203],\n",
       "          ...,\n",
       "          [-0.5126, -0.5826, -0.6702,  ..., -0.1450, -0.2150, -0.2675],\n",
       "          [-0.5126, -0.6001, -0.7227,  ..., -0.2850, -0.2850, -0.2850],\n",
       "          [-0.5126, -0.6001, -0.7402,  ..., -0.3550, -0.3200, -0.3025]],\n",
       " \n",
       "         [[ 0.4439,  0.4439,  0.4439,  ..., -0.6193, -0.5670, -0.5321],\n",
       "          [ 0.4439,  0.4439,  0.4439,  ..., -0.6018, -0.5147, -0.4624],\n",
       "          [ 0.4614,  0.4439,  0.4265,  ..., -0.5495, -0.4101, -0.3055],\n",
       "          ...,\n",
       "          [-0.6193, -0.6890, -0.7587,  ..., -0.6367, -0.6890, -0.7238],\n",
       "          [-0.6367, -0.7064, -0.8110,  ..., -0.7587, -0.7587, -0.7413],\n",
       "          [-0.6367, -0.7064, -0.8284,  ..., -0.8284, -0.7936, -0.7587]]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08c0215e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=64x64>,\n",
       " 'label': 0,\n",
       " 'pixel_values': tensor([[[2.1119, 2.1290, 2.1633,  ..., 2.2318, 2.2318, 2.2318],\n",
       "          [2.1804, 2.1633, 2.1633,  ..., 2.2318, 2.2318, 2.2318],\n",
       "          [2.2318, 2.2318, 2.1975,  ..., 2.2147, 2.2147, 2.2147],\n",
       "          ...,\n",
       "          [1.2728, 1.9578, 2.2489,  ..., 1.6838, 1.6667, 1.5982],\n",
       "          [2.0777, 2.1119, 1.8208,  ..., 2.1462, 2.0777, 1.9920],\n",
       "          [2.0263, 1.9578, 2.0777,  ..., 2.2318, 2.1975, 2.1975]],\n",
       " \n",
       "         [[2.3936, 2.4111, 2.4111,  ..., 2.4111, 2.4111, 2.4111],\n",
       "          [2.4286, 2.4111, 2.4111,  ..., 2.4111, 2.4111, 2.4111],\n",
       "          [2.4286, 2.4286, 2.4111,  ..., 2.3936, 2.3936, 2.3936],\n",
       "          ...,\n",
       "          [1.4657, 2.1660, 2.4286,  ..., 1.8508, 1.8333, 1.7633],\n",
       "          [2.4286, 2.4286, 2.1835,  ..., 2.3235, 2.3060, 2.2185],\n",
       "          [2.4286, 2.4286, 2.4286,  ..., 2.4111, 2.4286, 2.4286]],\n",
       " \n",
       "         [[2.5703, 2.5877, 2.6051,  ..., 2.6226, 2.6226, 2.6226],\n",
       "          [2.6226, 2.6051, 2.6051,  ..., 2.6226, 2.6226, 2.6226],\n",
       "          [2.6400, 2.6400, 2.6051,  ..., 2.6051, 2.6051, 2.6051],\n",
       "          ...,\n",
       "          [1.6640, 2.3611, 2.6400,  ..., 2.2043, 2.2217, 2.1520],\n",
       "          [2.6400, 2.6400, 2.3263,  ..., 2.6400, 2.6400, 2.5877],\n",
       "          [2.6400, 2.6051, 2.6400,  ..., 2.6400, 2.6400, 2.6400]]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17efd669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 64])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][\"pixel_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ed9fdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5c7bd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True , collate_fn=collate_fn)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=256, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b1823f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "\n",
    "model = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.DEFAULT)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, len(train_dataset.features[\"label\"].names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ac6b292",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e453e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c9a1d9a3ea443c2a97400afcaa7989c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 3.4840661308649556\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Loss: 2.9641834250496477\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Loss: 2.7713216058433514\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Loss: 2.6330488709842457\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Loss: 2.5439858345119544\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "num_epochs = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "    running_loss = 0.0\n",
    "    for i, data in tqdm(enumerate(train_loader), total=len(train_loader), leave=False):\n",
    "        inputs, labels = data[\"pixel_values\"], data[\"labels\"]\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Loss: {running_loss/len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e40fd2b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4bfc46dfd114015b96e2faa758de917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 43.43%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(valid_loader, desc=\"Validation\"):\n",
    "        inputs, labels = data[\"pixel_values\"], data[\"labels\"]\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255ebd37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
