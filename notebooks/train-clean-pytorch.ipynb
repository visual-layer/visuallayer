{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Dataset and Train a PyTorch Model\n",
    "In this Jupyter notebook, we will explore the implementation of a modified version of the `ImageFolder` dataset from the PyTorch `torchvision` package. This modified dataset filters out samples whose filenames are listed in a given CSV file. You obtain the CSV file by running fastdup (see [this notebook](./analyze.ipynb)) or dropping us an email at info@visual-layer.com .\n",
    "\n",
    "<!--<badge>--><a href=\"https://colab.research.google.com/github/visual-layer/vl-datasets/blob/master/notebooks/train-clean-pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><!--</badge>-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation & Setting Up\n",
    "\n",
    "First, we need to install the fastdup and matplotlib libraries. Run the following command in your Jupyter notebook to install them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U torch torchvision pandas\n",
    "!git clone https://github.com/visual-layer/vl-datasets && cd vl-datasets && pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download foods-101 Dataset\n",
    "Next, we need to download the dataset. For this tutorial, we will use the foods-101 dataset. Run the following commands in your Jupyter notebook to download and extract the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz\n",
    "!tar -xf food-101.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you're done extracting the resulting directory should look like this\n",
    "\n",
    "```\n",
    "food-101/\n",
    "├── images\n",
    "│   ├── apple_pie\n",
    "│   │   ├── 0001.jpg\n",
    "│   │   └── 0002.jpg\n",
    "│   ├── baby_back_ribs\n",
    "│   ├── baklava\n",
    "│   ├── ...\n",
    "│   ├── ...\n",
    "│   ├── ...\n",
    "│   └── waffles\n",
    "├── meta\n",
    "|   ├── classes.txt\n",
    "│   ├── labels.txt\n",
    "│   ├── test.json\n",
    "│   ├── test.txt\n",
    "│   ├── train.json\n",
    "│   └── train.txt\n",
    "├── license_agreement.txt\n",
    "└── README.txt\n",
    "```\n",
    "\n",
    "The train-test split information is contained in the `meta/` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structuring Data\n",
    "\n",
    "Let's take the above folder structure and turn it into a structure we can readily use with Torchvision.\n",
    "We will reorganize the above into the following folder structure:\n",
    "\n",
    "```\n",
    "food-101/\n",
    "├── train\n",
    "│   ├── apple_pie\n",
    "│   │   ├── 0001.jpg\n",
    "│   │   └── 0002.jpg\n",
    "│   ├── baby_back_ribs\n",
    "│   ├── baklava\n",
    "│   ├── ...\n",
    "│   ├── ...\n",
    "│   ├── ...\n",
    "│   └── waffles\n",
    "├── test\n",
    "│   ├── apple_pie\n",
    "│   │   ├── 01000.jpg\n",
    "│   │   └── 01001.jpg\n",
    "│   ├── baby_back_ribs\n",
    "│   ├── baklava\n",
    "│   ├── ...\n",
    "│   ├── ...\n",
    "│   ├── ...\n",
    "│   └── waffles\n",
    "├── meta\n",
    "|   ├── classes.txt\n",
    "│   ├── labels.txt\n",
    "│   ├── test.json\n",
    "│   ├── test.txt\n",
    "│   ├── train.json\n",
    "│   └── train.txt\n",
    "├── license_agreement.txt\n",
    "└── README.txt\n",
    "\n",
    "```\n",
    "\n",
    "In order to do that let's run the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "food-101/images\n",
      "food-101/images/hummus\n",
      "food-101/images/huevos_rancheros\n",
      "food-101/images/chocolate_cake\n",
      "food-101/images/beignets\n",
      "food-101/images/mussels\n",
      "food-101/images/sushi\n",
      "food-101/images/cheese_plate\n",
      "food-101/images/filet_mignon\n",
      "food-101/images/cheesecake\n",
      "food-101/images/dumplings\n",
      "food-101/images/greek_salad\n",
      "food-101/images/foie_gras\n",
      "food-101/images/macaroni_and_cheese\n",
      "food-101/images/breakfast_burrito\n",
      "food-101/images/bibimbap\n",
      "food-101/images/onion_rings\n",
      "food-101/images/pancakes\n",
      "food-101/images/waffles\n",
      "food-101/images/french_toast\n",
      "food-101/images/paella\n",
      "food-101/images/pizza\n",
      "food-101/images/oysters\n",
      "food-101/images/club_sandwich\n",
      "food-101/images/seaweed_salad\n",
      "food-101/images/nachos\n",
      "food-101/images/tacos\n",
      "food-101/images/creme_brulee\n",
      "food-101/images/macarons\n",
      "food-101/images/chicken_wings\n",
      "food-101/images/fried_calamari\n",
      "food-101/images/baby_back_ribs\n",
      "food-101/images/garlic_bread\n",
      "food-101/images/peking_duck\n",
      "food-101/images/deviled_eggs\n",
      "food-101/images/eggs_benedict\n",
      "food-101/images/beef_carpaccio\n",
      "food-101/images/shrimp_and_grits\n",
      "food-101/images/baklava\n",
      "food-101/images/cannoli\n",
      "food-101/images/hot_and_sour_soup\n",
      "food-101/images/chicken_quesadilla\n",
      "food-101/images/spaghetti_carbonara\n",
      "food-101/images/spaghetti_bolognese\n",
      "food-101/images/ceviche\n",
      "food-101/images/lobster_roll_sandwich\n",
      "food-101/images/poutine\n",
      "food-101/images/takoyaki\n",
      "food-101/images/panna_cotta\n",
      "food-101/images/bread_pudding\n",
      "food-101/images/apple_pie\n",
      "food-101/images/grilled_salmon\n",
      "food-101/images/croque_madame\n",
      "food-101/images/hamburger\n",
      "food-101/images/pho\n",
      "food-101/images/pad_thai\n",
      "food-101/images/lobster_bisque\n",
      "food-101/images/tiramisu\n",
      "food-101/images/guacamole\n",
      "food-101/images/pork_chop\n",
      "food-101/images/french_onion_soup\n",
      "food-101/images/carrot_cake\n",
      "food-101/images/scallops\n",
      "food-101/images/caprese_salad\n",
      "food-101/images/caesar_salad\n",
      "food-101/images/cup_cakes\n",
      "food-101/images/crab_cakes\n",
      "food-101/images/beef_tartare\n",
      "food-101/images/edamame\n",
      "food-101/images/fried_rice\n",
      "food-101/images/tuna_tartare\n",
      "food-101/images/prime_rib\n",
      "food-101/images/steak\n",
      "food-101/images/chocolate_mousse\n",
      "food-101/images/clam_chowder\n",
      "food-101/images/miso_soup\n",
      "food-101/images/frozen_yogurt\n",
      "food-101/images/bruschetta\n",
      "food-101/images/red_velvet_cake\n",
      "food-101/images/strawberry_shortcake\n",
      "food-101/images/french_fries\n",
      "food-101/images/gyoza\n",
      "food-101/images/ice_cream\n",
      "food-101/images/omelette\n",
      "food-101/images/lasagna\n",
      "food-101/images/samosa\n",
      "food-101/images/pulled_pork_sandwich\n",
      "food-101/images/chicken_curry\n",
      "food-101/images/donuts\n",
      "food-101/images/falafel\n",
      "food-101/images/fish_and_chips\n",
      "food-101/images/risotto\n",
      "food-101/images/sashimi\n",
      "food-101/images/churros\n",
      "food-101/images/gnocchi\n",
      "food-101/images/hot_dog\n",
      "food-101/images/spring_rolls\n",
      "food-101/images/ramen\n",
      "food-101/images/grilled_cheese_sandwich\n",
      "food-101/images/escargots\n",
      "food-101/images/beet_salad\n",
      "food-101/images/ravioli\n",
      "food-101/images\n",
      "food-101/images/hummus\n",
      "food-101/images/huevos_rancheros\n",
      "food-101/images/chocolate_cake\n",
      "food-101/images/beignets\n",
      "food-101/images/mussels\n",
      "food-101/images/sushi\n",
      "food-101/images/cheese_plate\n",
      "food-101/images/filet_mignon\n",
      "food-101/images/cheesecake\n",
      "food-101/images/dumplings\n",
      "food-101/images/greek_salad\n",
      "food-101/images/foie_gras\n",
      "food-101/images/macaroni_and_cheese\n",
      "food-101/images/breakfast_burrito\n",
      "food-101/images/bibimbap\n",
      "food-101/images/onion_rings\n",
      "food-101/images/pancakes\n",
      "food-101/images/waffles\n",
      "food-101/images/french_toast\n",
      "food-101/images/paella\n",
      "food-101/images/pizza\n",
      "food-101/images/oysters\n",
      "food-101/images/club_sandwich\n",
      "food-101/images/seaweed_salad\n",
      "food-101/images/nachos\n",
      "food-101/images/tacos\n",
      "food-101/images/creme_brulee\n",
      "food-101/images/macarons\n",
      "food-101/images/chicken_wings\n",
      "food-101/images/fried_calamari\n",
      "food-101/images/baby_back_ribs\n",
      "food-101/images/garlic_bread\n",
      "food-101/images/peking_duck\n",
      "food-101/images/deviled_eggs\n",
      "food-101/images/eggs_benedict\n",
      "food-101/images/beef_carpaccio\n",
      "food-101/images/shrimp_and_grits\n",
      "food-101/images/baklava\n",
      "food-101/images/cannoli\n",
      "food-101/images/hot_and_sour_soup\n",
      "food-101/images/chicken_quesadilla\n",
      "food-101/images/spaghetti_carbonara\n",
      "food-101/images/spaghetti_bolognese\n",
      "food-101/images/ceviche\n",
      "food-101/images/lobster_roll_sandwich\n",
      "food-101/images/poutine\n",
      "food-101/images/takoyaki\n",
      "food-101/images/panna_cotta\n",
      "food-101/images/bread_pudding\n",
      "food-101/images/apple_pie\n",
      "food-101/images/grilled_salmon\n",
      "food-101/images/croque_madame\n",
      "food-101/images/hamburger\n",
      "food-101/images/pho\n",
      "food-101/images/pad_thai\n",
      "food-101/images/lobster_bisque\n",
      "food-101/images/tiramisu\n",
      "food-101/images/guacamole\n",
      "food-101/images/pork_chop\n",
      "food-101/images/french_onion_soup\n",
      "food-101/images/carrot_cake\n",
      "food-101/images/scallops\n",
      "food-101/images/caprese_salad\n",
      "food-101/images/caesar_salad\n",
      "food-101/images/cup_cakes\n",
      "food-101/images/crab_cakes\n",
      "food-101/images/beef_tartare\n",
      "food-101/images/edamame\n",
      "food-101/images/fried_rice\n",
      "food-101/images/tuna_tartare\n",
      "food-101/images/prime_rib\n",
      "food-101/images/steak\n",
      "food-101/images/chocolate_mousse\n",
      "food-101/images/clam_chowder\n",
      "food-101/images/miso_soup\n",
      "food-101/images/frozen_yogurt\n",
      "food-101/images/bruschetta\n",
      "food-101/images/red_velvet_cake\n",
      "food-101/images/strawberry_shortcake\n",
      "food-101/images/french_fries\n",
      "food-101/images/gyoza\n",
      "food-101/images/ice_cream\n",
      "food-101/images/omelette\n",
      "food-101/images/lasagna\n",
      "food-101/images/samosa\n",
      "food-101/images/pulled_pork_sandwich\n",
      "food-101/images/chicken_curry\n",
      "food-101/images/donuts\n",
      "food-101/images/falafel\n",
      "food-101/images/fish_and_chips\n",
      "food-101/images/risotto\n",
      "food-101/images/sashimi\n",
      "food-101/images/churros\n",
      "food-101/images/gnocchi\n",
      "food-101/images/hot_dog\n",
      "food-101/images/spring_rolls\n",
      "food-101/images/ramen\n",
      "food-101/images/grilled_cheese_sandwich\n",
      "food-101/images/escargots\n",
      "food-101/images/beet_salad\n",
      "food-101/images/ravioli\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "class_to_ix = {}\n",
    "ix_to_class = {}\n",
    "with open('food-101/meta/classes.txt', 'r') as txt:\n",
    "    classes = [l.strip() for l in txt.readlines()]\n",
    "    class_to_ix = dict(zip(classes, range(len(classes))))\n",
    "    ix_to_class = dict(zip(range(len(classes)), classes))\n",
    "    class_to_ix = {v: k for k, v in ix_to_class.items()}\n",
    "sorted_class_to_ix = collections.OrderedDict(sorted(class_to_ix.items()))\n",
    "\n",
    "# Only split files if haven't already\n",
    "if not os.path.isdir('./food-101/test') and not os.path.isdir('./food-101/train'):\n",
    "\n",
    "    def copytree(src, dst, symlinks = False, ignore = None):\n",
    "        if not os.path.exists(dst):\n",
    "            os.makedirs(dst)\n",
    "            shutil.copystat(src, dst)\n",
    "        lst = os.listdir(src)\n",
    "        if ignore:\n",
    "            excl = ignore(src, lst)\n",
    "            lst = [x for x in lst if x not in excl]\n",
    "        for item in lst:\n",
    "            s = os.path.join(src, item)\n",
    "            d = os.path.join(dst, item)\n",
    "            if symlinks and os.path.islink(s):\n",
    "                if os.path.lexists(d):\n",
    "                    os.remove(d)\n",
    "                os.symlink(os.readlink(s), d)\n",
    "                try:\n",
    "                    st = os.lstat(s)\n",
    "                    mode = stat.S_IMODE(st.st_mode)\n",
    "                    os.lchmod(d, mode)\n",
    "                except:\n",
    "                    pass # lchmod not available\n",
    "            elif os.path.isdir(s):\n",
    "                copytree(s, d, symlinks, ignore)\n",
    "            else:\n",
    "                shutil.copy2(s, d)\n",
    "\n",
    "    def generate_dir_file_map(path):\n",
    "        dir_files = defaultdict(list)\n",
    "        with open(path, 'r') as txt:\n",
    "            files = [l.strip() for l in txt.readlines()]\n",
    "            for f in files:\n",
    "                dir_name, id = f.split('/')\n",
    "                dir_files[dir_name].append(id + '.jpg')\n",
    "        return dir_files\n",
    "\n",
    "    train_dir_files = generate_dir_file_map('food-101/meta/train.txt')\n",
    "    test_dir_files = generate_dir_file_map('food-101/meta/test.txt')\n",
    "\n",
    "\n",
    "    def ignore_train(d, filenames):\n",
    "        print(d)\n",
    "        subdir = d.split('/')[-1]\n",
    "        to_ignore = train_dir_files[subdir]\n",
    "        return to_ignore\n",
    "\n",
    "    def ignore_test(d, filenames):\n",
    "        print(d)\n",
    "        subdir = d.split('/')[-1]\n",
    "        to_ignore = test_dir_files[subdir]\n",
    "        return to_ignore\n",
    "\n",
    "    copytree('food-101/images', 'food-101/test', ignore=ignore_train)\n",
    "    copytree('food-101/images', 'food-101/train', ignore=ignore_test)\n",
    "    \n",
    "else:\n",
    "    print('Train/Test files already copied into separate folders.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "We will use the following libraries in this tutorial. Import them in your Jupyter notebook by running the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the preprocessing transforms\n",
    "We define the preprocessing transforms for the dataset. We have two transforms: `train_transform` and `valid_transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "valid_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a custom class FilteredDataset\n",
    "We define a custom class `FilteredDataset` that extends `ImageFolder` class. \n",
    "This class will allow us to exclude files from the dataset with a `.csv` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vl_datasets import FilteredDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exclude files\n",
    "Using the custom `FilteredDataset` class, we can conveniently exclude the files specified in the `.csv` files from being loaaded into the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Samples: 75750 in food-101/train\n",
      "Excluded: 498 in food-101/train\n",
      "Cleaned Samples: 75343 in food-101/train\n"
     ]
    }
   ],
   "source": [
    "train_dataset = FilteredDataset(\"food-101/train\", \"food_101_vl-datasets_analysis.csv\", transform=train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Samples: 25250 in food-101/test\n",
      "Excluded: 498 in food-101/test\n",
      "Cleaned Samples: 25159 in food-101/test\n"
     ]
    }
   ],
   "source": [
    "valid_dataset = FilteredDataset(\"food-101/test\", \"food_101_vl-datasets_analysis.csv\", transform=train_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also view the exclude files with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>reason</th>\n",
       "      <th>value</th>\n",
       "      <th>prototype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>food-101/train/apple_pie/1487150.jpg</td>\n",
       "      <td>Duplicate</td>\n",
       "      <td>0.9662</td>\n",
       "      <td>apple_pie/1486972.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>food-101/train/apple_pie/3324492.jpg</td>\n",
       "      <td>Duplicate</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>apple_pie/2106005.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>food-101/train/apple_pie/3670966.jpg</td>\n",
       "      <td>Duplicate</td>\n",
       "      <td>0.9879</td>\n",
       "      <td>apple_pie/3670548.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>food-101/train/apple_pie/839845.jpg</td>\n",
       "      <td>Duplicate</td>\n",
       "      <td>0.9964</td>\n",
       "      <td>apple_pie/839808.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>food-101/train/baby_back_ribs/2306066.jpg</td>\n",
       "      <td>Duplicate</td>\n",
       "      <td>0.9862</td>\n",
       "      <td>baby_back_ribs/2306008.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>food-101/train/sashimi/241368.jpg</td>\n",
       "      <td>Dark</td>\n",
       "      <td>15.7813</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>food-101/train/scallops/3314913.jpg</td>\n",
       "      <td>Dark</td>\n",
       "      <td>13.7173</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>food-101/train/spring_rolls/182658.jpg</td>\n",
       "      <td>Dark</td>\n",
       "      <td>8.9502</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>food-101/train/bread_pudding/444890.jpg</td>\n",
       "      <td>File-Size</td>\n",
       "      <td>9715.0000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>food-101/train/breakfast_burrito/462294.jpg</td>\n",
       "      <td>File-Size</td>\n",
       "      <td>8693.0000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>513 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        filename     reason      value   \n",
       "0           food-101/train/apple_pie/1487150.jpg  Duplicate     0.9662  \\\n",
       "1           food-101/train/apple_pie/3324492.jpg  Duplicate     0.9817   \n",
       "2           food-101/train/apple_pie/3670966.jpg  Duplicate     0.9879   \n",
       "3            food-101/train/apple_pie/839845.jpg  Duplicate     0.9964   \n",
       "4      food-101/train/baby_back_ribs/2306066.jpg  Duplicate     0.9862   \n",
       "..                                           ...        ...        ...   \n",
       "508            food-101/train/sashimi/241368.jpg       Dark    15.7813   \n",
       "509          food-101/train/scallops/3314913.jpg       Dark    13.7173   \n",
       "510       food-101/train/spring_rolls/182658.jpg       Dark     8.9502   \n",
       "511      food-101/train/bread_pudding/444890.jpg  File-Size  9715.0000   \n",
       "512  food-101/train/breakfast_burrito/462294.jpg  File-Size  8693.0000   \n",
       "\n",
       "                      prototype  \n",
       "0         apple_pie/1486972.jpg  \n",
       "1         apple_pie/2106005.jpg  \n",
       "2         apple_pie/3670548.jpg  \n",
       "3          apple_pie/839808.jpg  \n",
       "4    baby_back_ribs/2306008.jpg  \n",
       "..                          ...  \n",
       "508                         NaN  \n",
       "509                         NaN  \n",
       "510                         NaN  \n",
       "511                         NaN  \n",
       "512                         NaN  \n",
       "\n",
       "[513 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.excluded_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=256, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model architecture\n",
    "Let's construct a basic convolutional model, Resnet18 from Torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.DEFAULT)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, len(train_dataset.classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "Now, let's write a simple training loop to train the model for 10 epochs on a GPU or CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1 - Loss: 2.407583218913967\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Loss: {running_loss/len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model\n",
    "Finally we evaluate the model on the validation set and prints it's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 54.3026352398744\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in valid_loader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
