{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Filter Dataset and Train a PyTorch Model\n",
        "In this Jupyter notebook, we will explore the implementation of a modified version of the `ImageFolder` dataset from the PyTorch `torchvision` package. This modified dataset filters out samples whose filenames are listed in a given CSV file. You obtain the CSV file by running fastdup (see [this notebook](./analyze.ipynb)) or dropping us an email at info@visual-layer.com .\n",
        "\n",
        "<!--<badge>--><a href=\"https://colab.research.google.com/github/visual-layer/vl-datasets/blob/master/notebooks/train-clean-pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><!--</badge>-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation & Setting Up\n",
        "\n",
        "First, we need to install the fastdup and matplotlib libraries. Run the following command in your Jupyter notebook to install them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -U torch torchvision pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download foods-101 Dataset\n",
        "Next, we need to download the dataset. For this tutorial, we will use the foods-101 dataset. Run the following commands in your Jupyter notebook to download and extract the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz\n",
        "!tar -xf food-101.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once you're done extracting the resulting directory should look like this\n",
        "\n",
        "```\n",
        "food-101/\n",
        "\u251c\u2500\u2500 images\n",
        "\u2502   \u251c\u2500\u2500 apple_pie\n",
        "\u2502   \u2502   \u251c\u2500\u2500 0001.jpg\n",
        "\u2502   \u2502   \u2514\u2500\u2500 0002.jpg\n",
        "\u2502   \u251c\u2500\u2500 baby_back_ribs\n",
        "\u2502   \u251c\u2500\u2500 baklava\n",
        "\u2502   \u251c\u2500\u2500 ...\n",
        "\u2502   \u251c\u2500\u2500 ...\n",
        "\u2502   \u251c\u2500\u2500 ...\n",
        "\u2502   \u2514\u2500\u2500 waffles\n",
        "\u251c\u2500\u2500 meta\n",
        "|   \u251c\u2500\u2500 classes.txt\n",
        "\u2502   \u251c\u2500\u2500 labels.txt\n",
        "\u2502   \u251c\u2500\u2500 test.json\n",
        "\u2502   \u251c\u2500\u2500 test.txt\n",
        "\u2502   \u251c\u2500\u2500 train.json\n",
        "\u2502   \u2514\u2500\u2500 train.txt\n",
        "\u251c\u2500\u2500 license_agreement.txt\n",
        "\u2514\u2500\u2500 README.txt\n",
        "```\n",
        "\n",
        "The train-test split information is contained in the `meta/` folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Structuring Data\n",
        "\n",
        "Let's take the above folder structure and turn it into a structure we can readily use with Torchvision.\n",
        "We will reorganize the above into the following folder structure:\n",
        "\n",
        "```\n",
        "food-101/\n",
        "\u251c\u2500\u2500 train\n",
        "\u2502   \u251c\u2500\u2500 apple_pie\n",
        "\u2502   \u2502   \u251c\u2500\u2500 0001.jpg\n",
        "\u2502   \u2502   \u2514\u2500\u2500 0002.jpg\n",
        "\u2502   \u251c\u2500\u2500 baby_back_ribs\n",
        "\u2502   \u251c\u2500\u2500 baklava\n",
        "\u2502   \u251c\u2500\u2500 ...\n",
        "\u2502   \u251c\u2500\u2500 ...\n",
        "\u2502   \u251c\u2500\u2500 ...\n",
        "\u2502   \u2514\u2500\u2500 waffles\n",
        "\u251c\u2500\u2500 test\n",
        "\u2502   \u251c\u2500\u2500 apple_pie\n",
        "\u2502   \u2502   \u251c\u2500\u2500 01000.jpg\n",
        "\u2502   \u2502   \u2514\u2500\u2500 01001.jpg\n",
        "\u2502   \u251c\u2500\u2500 baby_back_ribs\n",
        "\u2502   \u251c\u2500\u2500 baklava\n",
        "\u2502   \u251c\u2500\u2500 ...\n",
        "\u2502   \u251c\u2500\u2500 ...\n",
        "\u2502   \u251c\u2500\u2500 ...\n",
        "\u2502   \u2514\u2500\u2500 waffles\n",
        "\u251c\u2500\u2500 meta\n",
        "|   \u251c\u2500\u2500 classes.txt\n",
        "\u2502   \u251c\u2500\u2500 labels.txt\n",
        "\u2502   \u251c\u2500\u2500 test.json\n",
        "\u2502   \u251c\u2500\u2500 test.txt\n",
        "\u2502   \u251c\u2500\u2500 train.json\n",
        "\u2502   \u2514\u2500\u2500 train.txt\n",
        "\u251c\u2500\u2500 license_agreement.txt\n",
        "\u2514\u2500\u2500 README.txt\n",
        "\n",
        "```\n",
        "\n",
        "In order to do that let's run the following code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "import collections\n",
        "from collections import defaultdict\n",
        "import os\n",
        "\n",
        "class_to_ix = {}\n",
        "ix_to_class = {}\n",
        "with open('food-101/meta/classes.txt', 'r') as txt:\n",
        "    classes = [l.strip() for l in txt.readlines()]\n",
        "    class_to_ix = dict(zip(classes, range(len(classes))))\n",
        "    ix_to_class = dict(zip(range(len(classes)), classes))\n",
        "    class_to_ix = {v: k for k, v in ix_to_class.items()}\n",
        "sorted_class_to_ix = collections.OrderedDict(sorted(class_to_ix.items()))\n",
        "\n",
        "# Only split files if haven't already\n",
        "if not os.path.isdir('./food-101/test') and not os.path.isdir('./food-101/train'):\n",
        "\n",
        "    def copytree(src, dst, symlinks = False, ignore = None):\n",
        "        if not os.path.exists(dst):\n",
        "            os.makedirs(dst)\n",
        "            shutil.copystat(src, dst)\n",
        "        lst = os.listdir(src)\n",
        "        if ignore:\n",
        "            excl = ignore(src, lst)\n",
        "            lst = [x for x in lst if x not in excl]\n",
        "        for item in lst:\n",
        "            s = os.path.join(src, item)\n",
        "            d = os.path.join(dst, item)\n",
        "            if symlinks and os.path.islink(s):\n",
        "                if os.path.lexists(d):\n",
        "                    os.remove(d)\n",
        "                os.symlink(os.readlink(s), d)\n",
        "                try:\n",
        "                    st = os.lstat(s)\n",
        "                    mode = stat.S_IMODE(st.st_mode)\n",
        "                    os.lchmod(d, mode)\n",
        "                except:\n",
        "                    pass # lchmod not available\n",
        "            elif os.path.isdir(s):\n",
        "                copytree(s, d, symlinks, ignore)\n",
        "            else:\n",
        "                shutil.copy2(s, d)\n",
        "\n",
        "    def generate_dir_file_map(path):\n",
        "        dir_files = defaultdict(list)\n",
        "        with open(path, 'r') as txt:\n",
        "            files = [l.strip() for l in txt.readlines()]\n",
        "            for f in files:\n",
        "                dir_name, id = f.split('/')\n",
        "                dir_files[dir_name].append(id + '.jpg')\n",
        "        return dir_files\n",
        "\n",
        "    train_dir_files = generate_dir_file_map('food-101/meta/train.txt')\n",
        "    test_dir_files = generate_dir_file_map('food-101/meta/test.txt')\n",
        "\n",
        "\n",
        "    def ignore_train(d, filenames):\n",
        "        print(d)\n",
        "        subdir = d.split('/')[-1]\n",
        "        to_ignore = train_dir_files[subdir]\n",
        "        return to_ignore\n",
        "\n",
        "    def ignore_test(d, filenames):\n",
        "        print(d)\n",
        "        subdir = d.split('/')[-1]\n",
        "        to_ignore = test_dir_files[subdir]\n",
        "        return to_ignore\n",
        "\n",
        "    copytree('food-101/images', 'food-101/test', ignore=ignore_train)\n",
        "    copytree('food-101/images', 'food-101/train', ignore=ignore_test)\n",
        "    \n",
        "else:\n",
        "    print('Train/Test files already copied into separate folders.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports\n",
        "We will use the following libraries in this tutorial. Import them in your Jupyter notebook by running the following commands:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "import torchvision\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the preprocessing transforms\n",
        "We define the preprocessing transforms for the dataset. We have two transforms: `train_transform` and `valid_transform`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "valid_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define a custom class FilteredDataset\n",
        "We define a custom class `FilteredDataset` that extends `ImageFolder` class. \n",
        "This class will allow us to exclude files from the dataset with a `.csv` file. \n",
        "\n",
        "Here is the code for the `FilteredDataset` class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "class FilteredDataset(ImageFolder):\n",
        "    \"\"\"\n",
        "    A modified version of torchvision.datasets.ImageFolder that filters out samples whose filenames\n",
        "    are listed in a given CSV file.\n",
        "\n",
        "    See: https://pytorch.org/vision/main/generated/torchvision.datasets.ImageFolder.html\n",
        "\n",
        "    Args:\n",
        "        root_dir (string): Root directory path of the dataset.\n",
        "        csv_path (string, optional): Path to a CSV file containing a list of excluded filenames.\n",
        "                                     Default: None.\n",
        "        transform (callable, optional): A function/transform that takes in a PIL image and returns a\n",
        "                                         transformed version. E.g, ``transforms.RandomCrop``\n",
        "                                         Default: None.\n",
        "        target_transform (callable, optional): A function/transform that takes in the target and\n",
        "                                                transforms it. Default: None.\n",
        "\n",
        "    Example usage:\n",
        "\n",
        "        # Load the dataset and exclude certain samples\n",
        "        dataset = FilteredDataset(\"dataset/images\", \"files-to-exclude.csv\", transform=transforms.ToTensor())\n",
        "\n",
        "        # Create a dataloader\n",
        "        dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, csv_path=None, transform=None, target_transform=None):\n",
        "        root_dir = Path(root_dir)\n",
        "        super().__init__(root_dir, transform=transform, target_transform=target_transform)\n",
        "\n",
        "        if csv_path:\n",
        "            self.excluded_files = pd.read_csv(csv_path, header=0)\n",
        "            self.excluded_files['filename'] = self.excluded_files['filename'].apply(lambda x: str(root_dir) + \"/\" + x )\n",
        "            self.excluded_filenames = set(self.excluded_files['filename'])\n",
        " \n",
        "            print(f\"Original Samples: {len(self.samples)} in {root_dir}\")\n",
        "            print(f\"Excluded: {len(self.excluded_filenames)} in {root_dir}\")\n",
        "            excluded_indices = [i for i, (path, _) in enumerate(self.samples) if path in self.excluded_filenames]\n",
        "            self.samples = [sample for i, sample in enumerate(self.samples) if i not in excluded_indices]\n",
        "            self.targets = [target for i, target in enumerate(self.targets) if i not in excluded_indices]\n",
        "            print(f\"Cleaned Samples: {len(self.samples)} in {root_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exclude files\n",
        "Using the custom `FilteredDataset` class, we can conveniently exclude the files specified in the `.csv` files from being loaaded into the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Samples: 75750 in food-101/train\n",
            "Excluded: 498 in food-101/train\n",
            "Cleaned Samples: 75343 in food-101/train\n"
          ]
        }
      ],
      "source": [
        "train_dataset = FilteredDataset(\"food-101/train\", \"food_101_vl-datasets_analysis.csv\", transform=train_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Samples: 25250 in food-101/test\n",
            "Excluded: 498 in food-101/test\n",
            "Cleaned Samples: 25159 in food-101/test\n"
          ]
        }
      ],
      "source": [
        "valid_dataset = FilteredDataset(\"food-101/test\", \"food_101_vl-datasets_analysis.csv\", transform=train_transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also view the exclude files with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>reason</th>\n",
              "      <th>value</th>\n",
              "      <th>prototype</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>food-101/train/apple_pie/1487150.jpg</td>\n",
              "      <td>Duplicate</td>\n",
              "      <td>0.9662</td>\n",
              "      <td>apple_pie/1486972.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>food-101/train/apple_pie/3324492.jpg</td>\n",
              "      <td>Duplicate</td>\n",
              "      <td>0.9817</td>\n",
              "      <td>apple_pie/2106005.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>food-101/train/apple_pie/3670966.jpg</td>\n",
              "      <td>Duplicate</td>\n",
              "      <td>0.9879</td>\n",
              "      <td>apple_pie/3670548.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>food-101/train/apple_pie/839845.jpg</td>\n",
              "      <td>Duplicate</td>\n",
              "      <td>0.9964</td>\n",
              "      <td>apple_pie/839808.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>food-101/train/baby_back_ribs/2306066.jpg</td>\n",
              "      <td>Duplicate</td>\n",
              "      <td>0.9862</td>\n",
              "      <td>baby_back_ribs/2306008.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>508</th>\n",
              "      <td>food-101/train/sashimi/241368.jpg</td>\n",
              "      <td>Dark</td>\n",
              "      <td>15.7813</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>509</th>\n",
              "      <td>food-101/train/scallops/3314913.jpg</td>\n",
              "      <td>Dark</td>\n",
              "      <td>13.7173</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>510</th>\n",
              "      <td>food-101/train/spring_rolls/182658.jpg</td>\n",
              "      <td>Dark</td>\n",
              "      <td>8.9502</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>511</th>\n",
              "      <td>food-101/train/bread_pudding/444890.jpg</td>\n",
              "      <td>File-Size</td>\n",
              "      <td>9715.0000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>512</th>\n",
              "      <td>food-101/train/breakfast_burrito/462294.jpg</td>\n",
              "      <td>File-Size</td>\n",
              "      <td>8693.0000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>513 rows \u00d7 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        filename     reason      value   \n",
              "0           food-101/train/apple_pie/1487150.jpg  Duplicate     0.9662  \\\n",
              "1           food-101/train/apple_pie/3324492.jpg  Duplicate     0.9817   \n",
              "2           food-101/train/apple_pie/3670966.jpg  Duplicate     0.9879   \n",
              "3            food-101/train/apple_pie/839845.jpg  Duplicate     0.9964   \n",
              "4      food-101/train/baby_back_ribs/2306066.jpg  Duplicate     0.9862   \n",
              "..                                           ...        ...        ...   \n",
              "508            food-101/train/sashimi/241368.jpg       Dark    15.7813   \n",
              "509          food-101/train/scallops/3314913.jpg       Dark    13.7173   \n",
              "510       food-101/train/spring_rolls/182658.jpg       Dark     8.9502   \n",
              "511      food-101/train/bread_pudding/444890.jpg  File-Size  9715.0000   \n",
              "512  food-101/train/breakfast_burrito/462294.jpg  File-Size  8693.0000   \n",
              "\n",
              "                      prototype  \n",
              "0         apple_pie/1486972.jpg  \n",
              "1         apple_pie/2106005.jpg  \n",
              "2         apple_pie/3670548.jpg  \n",
              "3          apple_pie/839808.jpg  \n",
              "4    baby_back_ribs/2306008.jpg  \n",
              "..                          ...  \n",
              "508                         NaN  \n",
              "509                         NaN  \n",
              "510                         NaN  \n",
              "511                         NaN  \n",
              "512                         NaN  \n",
              "\n",
              "[513 rows x 4 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset.excluded_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=256, shuffle=True, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the model architecture\n",
        "Let's construct a basic convolutional model, Resnet18 from Torchvision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "model = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.DEFAULT)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, len(train_dataset.classes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the loss function and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the model\n",
        "Now, let's write a simple training loop to train the model for 10 epochs on a GPU or CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Epoch 1 - Loss: 2.3990132137880487\n",
            "Epoch 2 - Loss: 1.8088141485796136\n",
            "Epoch 3 - Loss: 1.598961140341678\n",
            "Epoch 4 - Loss: 1.4813383158990892\n",
            "Epoch 5 - Loss: 1.3898559869345972\n",
            "Epoch 6 - Loss: 1.3187752719652854\n",
            "Epoch 7 - Loss: 1.258394802626917\n",
            "Epoch 8 - Loss: 1.1980765166929213\n",
            "Epoch 9 - Loss: 1.1490829083879115\n",
            "Epoch 10 - Loss: 1.1118874396307994\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} - Loss: {running_loss/len(train_loader)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate the model\n",
        "Finally we evaluate the model on the validation set and prints it's accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 69.50594220755993\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in valid_loader:\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Accuracy: {100 * correct / total}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}