{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class FilteredDataset(ImageFolder):\n",
    "    \"\"\"\n",
    "    A modified version of torchvision.datasets.ImageFolder that filters out samples whose filenames\n",
    "    are listed in a given CSV file.\n",
    "\n",
    "    See: https://pytorch.org/vision/main/generated/torchvision.datasets.ImageFolder.html\n",
    "\n",
    "    Args:\n",
    "        root_dir (string): Root directory path of the dataset.\n",
    "        csv_path (string, optional): Path to a CSV file containing a list of excluded filenames.\n",
    "                                     Default: None.\n",
    "        transform (callable, optional): A function/transform that takes in a PIL image and returns a\n",
    "                                         transformed version. E.g, ``transforms.RandomCrop``\n",
    "                                         Default: None.\n",
    "        target_transform (callable, optional): A function/transform that takes in the target and\n",
    "                                                transforms it. Default: None.\n",
    "\n",
    "    Example usage:\n",
    "\n",
    "        # Load the dataset and exclude certain samples\n",
    "        dataset = FilteredDataset('data/train_set', 'data/excluded_samples.csv', transform=transforms.ToTensor())\n",
    "\n",
    "        # Create a dataloader\n",
    "        dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, csv_path=None, transform=None, target_transform=None):\n",
    "        root_dir = Path(root_dir)\n",
    "        super().__init__(\n",
    "            root_dir, transform=transform, target_transform=target_transform\n",
    "        )\n",
    "\n",
    "        if csv_path:\n",
    "            self.excluded_files = set(\n",
    "                pd.read_csv(csv_path, header=0, names=[\"filename\"])[\"filename\"]\n",
    "            )\n",
    "            print(f\"Original Samples: {len(self.samples)} in {root_dir}\")\n",
    "            print(f\"Excluded: {len(self.excluded_files)} in {root_dir}\")\n",
    "            self.samples = [\n",
    "                (path, target)\n",
    "                for path, target in self.samples\n",
    "                if Path(path) not in self.excluded_files\n",
    "            ]\n",
    "            print(f\"Cleaned Samples: {len(self.samples)} in {root_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the preprocessing transforms\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomResizedCrop(150),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "valid_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(156),\n",
    "        transforms.CenterCrop(150),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Samples: 14034 in data/train\n",
      "Excluded: 11 in data/train\n",
      "Cleaned Samples: 14034 in data/train\n",
      "Original Samples: 3000 in data/val\n",
      "Excluded: 11 in data/val\n",
      "Cleaned Samples: 3000 in data/val\n"
     ]
    }
   ],
   "source": [
    "train_dataset = FilteredDataset(\n",
    "    \"data/train\", \"duplicates.csv\", transform=train_transform\n",
    ")\n",
    "valid_dataset = FilteredDataset(\"data/val\", \"duplicates.csv\", transform=valid_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data/train/forest/18807.jpg',\n",
       " 'data/train/forest/8689.jpg',\n",
       " 'data/train/mountain/15770.jpg',\n",
       " 'data/train/mountain/17775.jpg',\n",
       " 'data/train/mountain/19959.jpg',\n",
       " 'data/train/mountain/6518.jpg',\n",
       " 'data/train/mountain/7654.jpg',\n",
       " 'data/train/mountain/7865.jpg',\n",
       " 'data/train/sea/6337.jpg',\n",
       " 'data/train/street/1495.jpg',\n",
       " 'data/train/street/2764.jpg'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.excluded_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=96, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=96, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "model = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.DEFAULT)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, len(train_dataset.classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5569173758532725\n",
      "Epoch 2 - Loss: 0.43484584662784526\n",
      "Epoch 3 - Loss: 0.3997510220526027\n",
      "Epoch 4 - Loss: 0.3898249220888631\n",
      "Epoch 5 - Loss: 0.36313092424756005\n",
      "Epoch 6 - Loss: 0.34684148562603256\n",
      "Epoch 7 - Loss: 0.3528172503523275\n",
      "Epoch 8 - Loss: 0.3397237540913277\n",
      "Epoch 9 - Loss: 0.3298793699668378\n",
      "Epoch 10 - Loss: 0.3128994928330791\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Loss: {running_loss/len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.46666666666667\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in valid_loader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
