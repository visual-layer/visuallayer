{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Dataset and Train a PyTorch Model\n",
    "In this Jupyter notebook, we will explore the implementation of a modified version of the `ImageFolder` dataset from the PyTorch `torchvision` package. This modified dataset filters out samples whose filenames are listed in a given CSV file. You obtain the CSV file by running fastdup (see [this notebook](./analyze.ipynb)) or dropping us an email at info@visual-layer.com .\n",
    "\n",
    "<!--<badge>--><a href=\"https://colab.research.google.com/github/visual-layer/vl-datasets/blob/master/notebooks/train-clean-pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><!--</badge>-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation & Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastdup in /home/dnth/anaconda3/envs/fastdupv1/lib/python3.10/site-packages (0.915)\n",
      "Requirement already satisfied: matplotlib in /home/dnth/anaconda3/envs/fastdupv1/lib/python3.10/site-packages (3.7.1)\n",
      "Requirement already satisfied: tqdm in /home/dnth/anaconda3/envs/fastdupv1/lib/python3.10/site-packages (from fastdup) (4.65.0)\n",
      "Requirement already satisfied: numpy in /home/dnth/anaconda3/envs/fastdupv1/lib/python3.10/site-packages (from fastdup) (1.23.5)\n",
      "Requirement already satisfied: pandas in /home/dnth/anaconda3/envs/fastdupv1/lib/python3.10/site-packages (from fastdup) (1.5.3)\n",
      "Requirement already satisfied: opencv-python-headless in /home/dnth/anaconda3/envs/fastdupv1/lib/python3.10/site-packages (from fastdup) (4.7.0.72)\n",
      "Requirement already satisfied: pyyaml in /home/dnth/anaconda3/envs/fastdupv1/lib/python3.10/site-packages (from fastdup) (6.0)\n",
      "Requirement already satisfied: requests==2.28.1 in /home/dnth/anaconda3/envs/fastdupv1/lib/python3.10/site-packages (from fastdup) (2.28.1)\n",
      "Requirement already satisfied: pillow in /home/dnth/anaconda3/envs/fastdupv1/lib/python3.10/site-packages (from fastdup) (9.5.0)\n",
      "Requirement already satisfied: packaging in /home/dnth/anaconda3/envs/fastdupv1/lib/python3.10/site-packages (from fastdup) (23.0)\n",
      "Requirement already satisfied: sentry-sdk in /home/dnth/anaconda3/envs/fastdupv1/lib/python3.10/site-packages (from fastdup) (1.18.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/dnth/anaconda3/envs/fastdupv1/lib/python3.10/site-packages (from requests==2.28.1->fastdup) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dnth/anaconda3/envs/fastdupv1/lib/python3.10/site-packages (from requests==2.28.1->fastdup) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dnth/anaconda3/envs/fastdupv1/lib/python3.10/site-packages (from requests==2.28.1->fastdup) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/dnth/anaconda3/envs/fastdupv1/lib/python3.10/site-packages (from requests==2.28.1->fastdup) (1.26.15)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/dnth/anaconda3/envs/fastdupv1/lib/python3.10/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/dnth/anaconda3/envs/fastdupv1/lib/python3.10/site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/dnth/anaconda3/envs/fastdupv1/lib/python3.10/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/dnth/anaconda3/envs/fastdupv1/lib/python3.10/site-packages (from matplotlib) (4.39.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/dnth/anaconda3/envs/fastdupv1/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/dnth/anaconda3/envs/fastdupv1/lib/python3.10/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/dnth/anaconda3/envs/fastdupv1/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/dnth/anaconda3/envs/fastdupv1/lib/python3.10/site-packages (from pandas->fastdup) (2023.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U fastdup matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download foods-101 Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz\n",
    "!tar -xf food-101.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the preprocessing transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomResizedCrop(150),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "valid_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(156),\n",
    "        transforms.CenterCrop(150),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a custom class FilteredDataset\n",
    "The `FilteredDataset` class will allow us to exclude files from the dataset with a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilteredDataset(ImageFolder):\n",
    "    \"\"\"\n",
    "    A modified version of torchvision.datasets.ImageFolder that filters out samples whose filenames\n",
    "    are listed in a given CSV file.\n",
    "\n",
    "    See: https://pytorch.org/vision/main/generated/torchvision.datasets.ImageFolder.html\n",
    "\n",
    "    Args:\n",
    "        root_dir (string): Root directory path of the dataset.\n",
    "        csv_path (string, optional): Path to a CSV file containing a list of excluded filenames.\n",
    "                                     Default: None.\n",
    "        transform (callable, optional): A function/transform that takes in a PIL image and returns a\n",
    "                                         transformed version. E.g, ``transforms.RandomCrop``\n",
    "                                         Default: None.\n",
    "        target_transform (callable, optional): A function/transform that takes in the target and\n",
    "                                                transforms it. Default: None.\n",
    "\n",
    "    Example usage:\n",
    "\n",
    "        # Load the dataset and exclude certain samples\n",
    "        dataset = FilteredDataset('data/train_set', 'data/excluded_samples.csv', transform=transforms.ToTensor())\n",
    "\n",
    "        # Create a dataloader\n",
    "        dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, csv_path=None, transform=None, target_transform=None):\n",
    "        root_dir = Path(root_dir)\n",
    "        super().__init__(\n",
    "            root_dir, transform=transform, target_transform=target_transform\n",
    "        )\n",
    "\n",
    "        if csv_path:\n",
    "            self.excluded_files = set(\n",
    "                pd.read_csv(csv_path, header=0, names=[\"filename\"])[\"filename\"]\n",
    "            )\n",
    "            print(f\"Original Samples: {len(self.samples)} in {root_dir}\")\n",
    "            print(f\"Excluded: {len(self.excluded_files)} in {root_dir}\")\n",
    "            self.samples = [\n",
    "                (path, target)\n",
    "                for path, target in self.samples\n",
    "                if Path(path) not in self.excluded_files\n",
    "            ]\n",
    "            print(f\"Cleaned Samples: {len(self.samples)} in {root_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exclude files\n",
    "Using the custom `FilteredDataset` class, we can conveniently exclude the files specified in the `.csv` files from being loaaded into the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Samples: 14034 in data/train\n",
      "Excluded: 11 in data/train\n",
      "Cleaned Samples: 14034 in data/train\n",
      "Original Samples: 3000 in data/val\n",
      "Excluded: 11 in data/val\n",
      "Cleaned Samples: 3000 in data/val\n"
     ]
    }
   ],
   "source": [
    "train_dataset = FilteredDataset(\"data/train\", \"invalid.csv\", transform=train_transform)\n",
    "valid_dataset = FilteredDataset(\"data/val\", \"invalid.csv\", transform=valid_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also view the exclude files with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data/train/forest/18807.jpg',\n",
       " 'data/train/forest/8689.jpg',\n",
       " 'data/train/mountain/15770.jpg',\n",
       " 'data/train/mountain/17775.jpg',\n",
       " 'data/train/mountain/19959.jpg',\n",
       " 'data/train/mountain/6518.jpg',\n",
       " 'data/train/mountain/7654.jpg',\n",
       " 'data/train/mountain/7865.jpg',\n",
       " 'data/train/sea/6337.jpg',\n",
       " 'data/train/street/1495.jpg',\n",
       " 'data/train/street/2764.jpg'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.excluded_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=96, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=96, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.DEFAULT)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, len(train_dataset.classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5569173758532725\n",
      "Epoch 2 - Loss: 0.43484584662784526\n",
      "Epoch 3 - Loss: 0.3997510220526027\n",
      "Epoch 4 - Loss: 0.3898249220888631\n",
      "Epoch 5 - Loss: 0.36313092424756005\n",
      "Epoch 6 - Loss: 0.34684148562603256\n",
      "Epoch 7 - Loss: 0.3528172503523275\n",
      "Epoch 8 - Loss: 0.3397237540913277\n",
      "Epoch 9 - Loss: 0.3298793699668378\n",
      "Epoch 10 - Loss: 0.3128994928330791\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Loss: {running_loss/len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.46666666666667\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in valid_loader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
