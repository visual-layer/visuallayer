{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "class FilteredDataset(ImageFolder):\n",
    "    \"\"\"\n",
    "    A modified version of torchvision.datasets.ImageFolder that filters out samples whose filenames\n",
    "    are listed in a given CSV file.\n",
    "    \n",
    "    See: https://pytorch.org/vision/main/generated/torchvision.datasets.ImageFolder.html\n",
    "    \n",
    "    Args:\n",
    "        root_dir (string): Root directory path of the dataset.\n",
    "        csv_path (string, optional): Path to a CSV file containing a list of excluded filenames.\n",
    "                                     Default: None.\n",
    "        transform (callable, optional): A function/transform that takes in a PIL image and returns a\n",
    "                                         transformed version. E.g, ``transforms.RandomCrop``\n",
    "                                         Default: None.\n",
    "        target_transform (callable, optional): A function/transform that takes in the target and\n",
    "                                                transforms it. Default: None.\n",
    "\n",
    "    Example usage:\n",
    "\n",
    "        # Load the dataset and exclude certain samples\n",
    "        dataset = FilteredDataset('data/train_set', 'data/excluded_samples.csv', transform=transforms.ToTensor())\n",
    "\n",
    "        # Create a dataloader\n",
    "        dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, csv_path=None, transform=None, target_transform=None):\n",
    "        root_dir = Path(root_dir)\n",
    "        super().__init__(root_dir, transform=transform, target_transform=target_transform)\n",
    "\n",
    "        if csv_path:\n",
    "            self.excluded_files = set(pd.read_csv(csv_path, header=0, names=['filename'])['filename'])\n",
    "            print(f\"Original Samples: {len(self.samples)} in {root_dir}\")\n",
    "            print(f\"Excluded: {len(self.excluded_files)} in {root_dir}\")\n",
    "            self.samples = [(path, target) for path, target in self.samples if Path(path) not in self.excluded_files]\n",
    "            print(f\"Cleaned Samples: {len(self.samples)} in {root_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the preprocessing transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(150),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize(156),\n",
    "    transforms.CenterCrop(150),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Samples: 14034 in data/train\n",
      "Excluded: 11 in data/train\n",
      "Cleaned Samples: 14034 in data/train\n",
      "Original Samples: 3000 in data/val\n",
      "Excluded: 11 in data/val\n",
      "Cleaned Samples: 3000 in data/val\n"
     ]
    }
   ],
   "source": [
    "train_dataset = FilteredDataset('data/train', 'duplicates.csv', transform=train_transform)\n",
    "valid_dataset = FilteredDataset('data/val', 'duplicates.csv', transform=valid_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data/train/forest/18807.jpg',\n",
       " 'data/train/forest/8689.jpg',\n",
       " 'data/train/mountain/15770.jpg',\n",
       " 'data/train/mountain/17775.jpg',\n",
       " 'data/train/mountain/19959.jpg',\n",
       " 'data/train/mountain/6518.jpg',\n",
       " 'data/train/mountain/7654.jpg',\n",
       " 'data/train/mountain/7865.jpg',\n",
       " 'data/train/sea/6337.jpg',\n",
       " 'data/train/street/1495.jpg',\n",
       " 'data/train/street/2764.jpg'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.excluded_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=96, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=96, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dnth/anaconda3/envs/fastdupv1/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dnth/anaconda3/envs/fastdupv1/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Define the model architecture\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, len(train_dataset.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5492746412348585\n",
      "Epoch 2 - Loss: 0.4334449029090453\n",
      "Epoch 3 - Loss: 0.411048455505955\n",
      "Epoch 4 - Loss: 0.39966326100485666\n",
      "Epoch 5 - Loss: 0.3724620054368259\n",
      "Epoch 6 - Loss: 0.35996732409714033\n",
      "Epoch 7 - Loss: 0.34730940438857694\n",
      "Epoch 8 - Loss: 0.34328323308707903\n",
      "Epoch 9 - Loss: 0.32826554096069466\n",
      "Epoch 10 - Loss: 0.31970550099603173\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Loss: {running_loss/len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in valid_loader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
