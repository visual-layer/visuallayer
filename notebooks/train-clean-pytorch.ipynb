{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "class FilteredDataset(ImageFolder):\n",
    "    def __init__(self, root_dir, csv_path=None, transform=None, target_transform=None):\n",
    "        super().__init__(root_dir, transform=transform, target_transform=target_transform)\n",
    "\n",
    "        if csv_path:\n",
    "            # Load excluded filenames from csv\n",
    "            self.excluded_files = set(pd.read_csv(csv_path, header=0, names=['filename'])['filename'])\n",
    "            print(f\"Original Samples: {len(self.samples)} in {root_dir}\")\n",
    "            print(f\"Excluded: {len(self.excluded_files)} in {root_dir}\")\n",
    "            self.samples = [(path, target) for path, target in self.samples if path not in self.excluded_files]\n",
    "            print(f\"Cleaned Samples: {len(self.samples)} in {root_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the preprocessing transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(150),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize(156),\n",
    "    transforms.CenterCrop(150),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Samples: 14034 in data/train\n",
      "Excluded: 11 in data/train\n",
      "Cleaned Samples: 14023 in data/train\n",
      "Original Samples: 3000 in data/val\n",
      "Excluded: 11 in data/val\n",
      "Cleaned Samples: 3000 in data/val\n"
     ]
    }
   ],
   "source": [
    "train_dataset = FilteredDataset('data/train', 'duplicates.csv', transform=train_transform)\n",
    "valid_dataset = FilteredDataset('data/val', 'duplicates.csv', transform=valid_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data/train/forest/18807.jpg',\n",
       " 'data/train/forest/8689.jpg',\n",
       " 'data/train/mountain/15770.jpg',\n",
       " 'data/train/mountain/17775.jpg',\n",
       " 'data/train/mountain/19959.jpg',\n",
       " 'data/train/mountain/6518.jpg',\n",
       " 'data/train/mountain/7654.jpg',\n",
       " 'data/train/mountain/7865.jpg',\n",
       " 'data/train/sea/6337.jpg',\n",
       " 'data/train/street/1495.jpg',\n",
       " 'data/train/street/2764.jpg'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.excluded_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=96, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=96, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dnth/anaconda3/envs/fastdupv1/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dnth/anaconda3/envs/fastdupv1/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Define the model architecture\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, len(train_dataset.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5359818433620491\n",
      "Epoch 2 - Loss: 0.4541017396920392\n",
      "Epoch 3 - Loss: 0.4209156919296096\n",
      "Epoch 4 - Loss: 0.5001533070997316\n",
      "Epoch 5 - Loss: 0.45562055027809273\n",
      "Epoch 6 - Loss: 0.35864722961876666\n",
      "Epoch 7 - Loss: 0.37403261205371546\n",
      "Epoch 8 - Loss: 0.37237846679022524\n",
      "Epoch 9 - Loss: 0.40189469245826304\n",
      "Epoch 10 - Loss: 0.3582575438784904\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Loss: {running_loss/len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.76666666666667\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in valid_loader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
